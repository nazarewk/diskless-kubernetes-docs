
# Docelowa konfiguracja w sieci uczelnianej

Pełna konfiguracja Kubernetes znajduje się w folderze
`/pub/Linux/CoreOS/zetis/kubernetes`, możemy ją uruchomić z maszyny `ldap`,
znajdują się w nim foldery:

- `kubernetes-cluster` - moje repozytorium zawierające konfigurację i skrypty
  pozwalające uruchomić klaster,
- `boot` - skrót do folderu `kubernetes-cluster/zetis/WWW/boot` zawierającego
  konfigurację iPXE oraz Ignition:
  - `coreos.ign` - plik konfigurujący CoreOS, wygenerowany z pliku `coreos.yml`
    narzędziem do transpilacji konfiguracji [`ct`](https://github.com/coreos/container-linux-config-transpiler),
    narzędzie domyślnie nie jest skompilowane na FreeBSD i musimy uruchomić je
    z Linuxa,
- `log` - standardowe wyjście uruchamianych komend,

## Procedura uruchomienia klastra

1. Wchodzimy na maszynie `ldap` do folderu `/pub/Linux/CoreOS/zetis/kubernetes/kubernetes-cluster`
2. Upewniamy się, że nasz klucz SSH znajduje się w `boot/coreos.ign`
3. Włączamy maszyny-węzły wybierając z menu iPXE CoreOS -> Kubernetes lub
  wybierając w narzędziu `boot` bezpośrednio `coreos kub`.
  Następnie upewniamy się, że mamy bezhasłowy dostęp do tych maszyn, minimalna
  konfiguracja `~/.ssh/config`:
```
Host s?
  User admin

Host *
  User nazarewk
  IdentityFile ~/.ssh/id_rsa
  IdentitiesOnly yes

Host s? 10.146.255.*
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
```
 
4. Upewniamy się, że istnieje folder `kubespray/my_inventory`, jeżeli nie
  to go tworzymy kopiując domyślną konfigurację:
  ```
cp -rav kubespray/inventory kubespray/my_inventory
```
5. Otwieramy plik `inventory/inventory.cfg` i upewniamy się, że uruchomione maszyny są obecne w sekcji `[all]` oraz
  przypisane do odpowiednich ról: `[kube-master]` i `[etcd]` lub `[kube-node]`.
  Identyfikatorem maszyny jest pierwsze słowo w grupie `[all]`, przykładowa
  konfiguracja dla maszyn `s4`, `s5` i `s6` z jednym zarządcą to:
  
  ```
[all]
;s3  ip=10.146.225.3
s4  ip=10.146.225.4
s5  ip=10.146.225.5
s6  ip=10.146.225.6
;s7  ip=10.146.225.7
;s8  ip=10.146.225.8
;s9  ip=10.146.225.9
;sa  ip=10.146.225.10
;sb  ip=10.146.225.11
;sc  ip=10.146.225.12

[kube-master]
s4

[kube-node]
s5
s6

[etcd]
s4

[k8s-cluster:children]
kube-node
kube-master
```
  opcjonalnie możemy do każdego węzła dopisać `ansible_python_interpreter=/opt/bin/python`
  żeby ułatwić uruchamianie ansibla partiami
  
6. Upewniamy się, że plik `inventory/group_vars/all.yml` zawiera naszą konfigurację,
  minimalny przykład:
  
  ```yaml
cluster_name: zetis-kubernetes
bootstrap_os: coreos
kube_basic_auth: true
kubeconfig_localhost: true
kubectl_localhost: true
download_run_once: true
cert_management: "{{ 'vault' if groups.get('vault', None) else 'script' }}"
helm_enabled: true
helm_deployment_type: docker
kube_script_dir: /opt/bin/kubernetes-scripts
```

7. Uruchamiamy konfigurowanie maszyn `bin/setup-cluster` lub bez skryptu:
  ```bash
ldap% cd kubespray
ldap% ansible-playbook -i my_inventory/inventory.cfg cluster.yml -b -v
```
  
  Po około 10-20 minutach skrypt powinien zakończyć się wpisami pokroju:
  
  ```
...
PLAY RECAP ********
localhost                  : ok=2    changed=0    unreachable=0    failed=0   
s4                         : ok=281  changed=94   unreachable=0    failed=0   
s5                         : ok=346  changed=80   unreachable=0    failed=0   
s6                         : ok=186  changed=54   unreachable=0    failed=0   
...
```

8. Zweryfikujmy instalację:

```bash
ldap% bin/kubectl get nodes
NAME      STATUS    ROLES     AGE       VERSION
s4        Ready     master    2m        v1.9.1+coreos.0
s5        Ready     node      2m        v1.9.1+coreos.0
s6        Ready     node      2m        v1.9.1+coreos.0
```

## Sprawdzanie czy klaster działa

Wywołanie skryptu `bin/students nazarewk create` jest równoważne uruchomieniu
komendy `kubectl create -f nazarewk.yml`, gdzie plik `nazarewk.yml` to:
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: nazarewk
  labels:
    name: nazarewk
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nazarewk
  namespace: nazarewk
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nazarewk-admin-binding
  namespace: nazarewk
roleRef:
  kind: ClusterRole
  name: admin
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: nazarewk
```

Czyli:
- stworzeniu `Namespace`
- skonfigurowaniu dostępu za pomocą `ServiceAccount`
- przypisaniu domyślej `Role` o nazwie `admin` do `ServiceAccount` o nazwie
  `nazarewk` za pomocą `RoleBinding`
  
### Korzystanie z klastra jako student

- stwórzmy użytkownika z jego własnym `Namespace`
```
ldap% bin/students nazarewk create
namespace "nazarewk" created
serviceaccount "nazarewk" created
rolebinding "nazarewk-admin-binding" created
Tokens:
eyJhb<<<SKROCONY TOKEN>>>ahHfxU-TRw
ldap% bin/students
NAME          STATUS    AGE
default       Active    3m
kube-public   Active    3m
kube-system   Active    3m
nazarewk      Active    16s
```

  - skopiujmy token na `s2` z uruchomionym ubuntu:
```
  ldap% bin/student-tokens nazarewk | ssh nazarewk@s2 "cat > /tmp/token"
```

  - pobierzmy `kubectl`
```
s2% cd /tmp
s2% curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
sw% chmod +x kubectl
s2% sudo mv kubectl /usr/local/bin
s2% source <(kubectl completion zsh)
```

  - sprawdźmy czy mamy dostęp do klastra
```
s2% kubectl get nodes
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

  - skonfigurujmy kubectl (najprościej aliasem)
```
s2% alias kubectl='command kubectl -s "https://s4:6443" --insecure-skip-tls-verify=true --token="$(cat /tmp/token)" -n nazarewk'
```

  - zweryfikujmy brak dostępu do zasobów globalnych
```
s2% kubectl get nodes
Error from server (Forbidden): nodes is forbidden: User "system:serviceaccount:nazarewk:nazarewk" cannot list nodes at the cluster scope
```

  - stwórzmy deployment z przykładową aplikacją
```
s2% kubectl run echoserver \                                                                                                    
--image=gcr.io/google_containers/echoserver:1.4 \
--port=8080 \
--replicas=2
deployment "echoserver" created
s2% kubectl get deployments
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
echoserver   2         2         2            2           3m
s2% kubectl get pods
NAME                         READY     STATUS    RESTARTS   AGE
echoserver-7b9bbf6ff-22df4   1/1       Running   0          4m
echoserver-7b9bbf6ff-c6kbv   1/1       Running   0          4m
```

  - wystawmy port, żeby dostać się do aplikacji z poza klastra
```
s2% kubectl expose deployment echoserver --type=NodePort
service "echoserver" exposed
s2% kubectl describe services/echoserver | grep -e NodePort:
NodePort:                 <unset>  30100/TCP
s2% curl s4:30100
CLIENT VALUES:
client_address=10.233.107.64
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://s4:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=s4:30100
user-agent=curl/7.47.0
BODY:
-no body in request-
```

  - sprawdźmy, czy z ldap'a też mamy dostęp do aplikacji:
```
ldap% curl s4:30100
CLIENT VALUES:
client_address=10.233.107.64
command=GET
real path=/
query=nil
request_version=1.1
request_uri=http://s4:8080/

SERVER VALUES:
server_version=nginx: 1.10.0 - lua: 10001

HEADERS RECEIVED:
accept=*/*
host=s4:30100
user-agent=curl/7.58.0
BODY:
-no body in request-
```

  - usuńmy użytkownika
```
ldap% bin/students nazarewk delete
namespace "nazarewk" deleted
serviceaccount "nazarewk" deleted
rolebinding "nazarewk-admin-binding" deleted
Tokens:
Error from server (NotFound): serviceaccounts "nazarewk" not found
```

  - sprawdźmy czy coś zostało po koncie użytkownika
```
ldap% curl s4:30100
curl: (7) Failed to connect to s4 port 30100: Connection refused
ldap% bin/kubectl get namespace 
NAME          STATUS    AGE
default       Active    46m
kube-public   Active    46m
kube-system   Active    46m
```
