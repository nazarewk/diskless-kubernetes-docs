
## OpenShift Origin

Według [dokumentacji](https://docs.openshift.org/latest/getting_started/administrators.html)
istnieją dwie metody uruchamiania serwera, w Dockerze i bezpośrednio na
systemie Linux.

Zdecydowałem się uruchomić system w Dockerze ze względu na uniwersalność
rozwiązania. Komendą uruchamiającą serwer był:

```bash
docker run -d --name "origin" \
  --privileged --pid=host --net=host \
  -v /:/rootfs:ro \
  -v /var/run:/var/run:rw \
  -v /sys:/sys \
  -v /sys/fs/cgroup:/sys/fs/cgroup:rw \
  -v /var/lib/docker:/var/lib/docker:rw \
  -v /var/lib/origin/openshift.local.volumes:/var/lib/origin/openshift.local.volumes:rslave \
  openshift/origin start --public-master
```

W porównaniu do przykładu z dokumentacji dodany został argument `--public-master`
w celu uruchomienia interfejsu graficznego.

#### Sterowniki `cgroup`

Większość dystrybucji `Linuxa` (np. Arch, CoreOS, Fedora, Debian) nie
konfiguruje sterownika `cgroup` Dockera i korzysta z domyślnego `cgroupfs`,
wersję sterownika można zweryfikować komendą `docker info | grep -i cgroup`.

OpenShift korzysta ze sterownika `systemd`, a `kubelet` w trakcie rozruchu
weryfikuje zgodność sterowników, co skutkuje
[niekompatybilnością z domyślną konfiguracją Dockera](https://github.com/openshift/origin/issues/14766),
objawiającą się poniższym błędem:

    F0120 19:18:58.708005   25376 node.go:269] failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: "systemd" is different from docker cgroup driver: "cgroupfs"

Problem można rozwiązać dodając argumenty `--exec-opt native.cgroupdriver=systemd`
do komendy uruchamiającej główny proces Dockera.

#### Próba uruchomienia serwera na systemie Arch Linux

Po rozwiązniu powyższego problemu i udanym uruchomieniu ww. komendy rozruchowej
przeszedłem do dalszych etapów instalacji zgodnie z dokumentacją:

1. Uruchomiłem linię komend OpenShift i zalogowałem się jako użytkownik testowy:
```
$ docker exec -it origin bash
```
```
$ oc login
Username: test
Password: test
```
2. Stworzyłem nowy projekt:
```
$ oc new-project test
```

3. Pobrałem przykładową aplikację:
```
$ oc tag --source=docker openshift/deployment-example:v1 deployment-example:latest
```

4. Uruchomiłem instancję aplikacji:
```
$ oc new-app deployment-example:latest
```

Uruchomienie przykładowej zakończyło się poniższym błędem:
```
$ watch -n 5 oc status
In project test on server https://192.168.0.87:8443

svc/deployment-example - 172.30.52.184:8080
  dc/deployment-example deploys istag/deployment-example:latest 
    deployment #1 failed 1 minute ago: config change
```

Próba uruchomienia OpenShift Origin na Arch Linux zakończyła się niepowodzeniem.

#### Próba uruchomienia serwera na Fedora Atomic Host w VirtualBox

Następnie spróbowałem uruchomić OpenShift Origin na oficjalnie wspieranym
systemie operacyjnym `Fedora Atomic Host` z użyciem Vagranta w poniższej
konfiguracji `Vagrantfile`:
```ruby
# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "fedora/27-atomic-host"
  config.vm.box_check_update = false
  config.vm.network "forwarded_port", guest: 8443, host: 18443, host_ip: "127.0.0.1"
  config.vm.network "forwarded_port", guest: 8080, host: 18080, host_ip: "127.0.0.1"
  config.vm.provider "virtualbox" do |vb|
    vb.gui = false
    vb.memory = "8192"
  end
  config.vm.provision "shell", inline: <<-SHELL
  SHELL
end
```

Maszynę z lokalnym dyskiem uruchomiłem komendami `vagrant up`, a następnie
uzyskałem do niej dostęp komendą `vagrant ssh`. Kroki 1-4 były analogiczne do
uruchamiania na `Arch Linux`, po czym przeszedłem do kolejnych kroków z
dokumentacji:

5. Odczekałem aż przykładowa aplikacja się uruchomi i zweryfikowałem
  jej działanie:
```
$ watch -n 5 oc status
In project test on server https://10.0.2.15:8443

svc/deployment-example - 172.30.221.105:8080
  dc/deployment-example deploys istag/deployment-example:latest 
    deployment #1 deployed 3 seconds ago - 1 pod
```
```
$ curl http://172.30.221.105:8080 | grep v1
<div class="box"><h1>v1</h1><h2></h2></div>
```

6. Przeprowadziłem aktualizację aplikacji i ponownie zweryfikowałem jej
  działanie:
```
$ oc tag --source=docker openshift/deployment-example:v2 deployment-example:latest
Tag deployment-example:latest set to openshift/deployment-example:v2.
```
```
$ watch -n 5 oc status
In project test on server https://10.0.2.15:8443

svc/deployment-example - 172.30.221.105:8080
  dc/deployment-example deploys istag/deployment-example:latest 
    deployment #2 running for 8 seconds - 1 pod
    deployment #1 deployed 8 minutes ago - 1 pod
```
```
$ curl -s http://172.30.221.105:8080 | grep v2
<div class="box"><h1>v2</h1><h2></h2></div>
```

7. Próba dostania się do graficznego panelu administracyjnego zakończyła
się niepowodzeniem:
```
$ curl -k 'https://localhost:8443/console/'
missing service (service "webconsole" not found)
missing route (service "webconsole" not found)
```
Nie znalazłem rozwiązania powyższego problemu w internecie, ani na 
oficjalnym kanale `#openshift` na `irc.freenode.net`.

#### Wnioski

Panel administracyjny klastra `OpenShift Origin` jest jedyną znaczącą przewagą
nad `kubespray`. Reszta zarządzania klastrem odbywa się również za pomocą
repozytorium skryptów Ansibla ([w tym dodawanie kolejnych węzłów klastra](https://docs.openshift.com/enterprise/3.0/admin_guide/manage_nodes.html#adding-nodes)).

Z powodu braku dostępu do ww. panelu próbę uruchomienia `OpenShift Origin`
uznaję za nieudaną i odrzucam to narzędzie.
