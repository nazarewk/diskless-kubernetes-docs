
# Uruchamianie _Kubernetesa_ w laboratorium 225

## Przygotowanie węzłów _CoreOS_

Na wstępie przygotowałem _coreos.ipxe_ i _coreos.ign_ do rozruchu i bezhasłowego dostępu.

Po pierwsze stworzyłem Container Linux Config (plik _coreos.yml_) zawierający:

1. Tworzenie użytkownika _nazarewk_,
2. Nadanie mu praw do sudo i dockera (grupy _sudo_ i _docker_),
3. Dodanie dwóch kluczy: wewnętrznego uczelnianego i mojego używanego na codzień
  w celu zdalnego dostępu.

```yaml
passwd:
  users:
  - name: nazarewk
    groups: [sudo, docker]
    ssh_authorized_keys:
    - ssh-rsa ... nazarewk
    - ssh-rsa ... nazarewk@ldap.iem.pw.edu.pl
```

Następnie skompilowałem go do formatu Ignition narzędziem _ct_, skryptem
_bin/render-coreos_ z wykazu.

Przygotowałem skrypt _IPXE_ do uruchamiania CoreOS _zetis/WWW/boot/coreos.ipxe_.

Umieściłem skrypt w _/home/stud/nazarewk/WWW/boot_ i wskazałem go maszynom,
które będą węzłami:
```bash
sudo lab 's4 s5 s6 s8 s9' boot http://vol/~nazarewk/boot/coreos.ipxe 
```


## Przeszkody związane z uruchamianiem skryptów na uczelnianym Ubuntu

### Brak virtualenv'a
Moje skrypty nie przewidywały braku _virtualenva_, więc musiałem ręcznie
zainstalować go komendą `apt-get install virtualenv`.
Dodałem ten krok do skryptu _setup-packages_.

### Klonowanie repozytorium bez logowania

W celu umożliwienia anonimowego klonowania repozytorium z Githuba,
zmieniłem protokół z _git_ na _https_:

    git clone https://github.com/nazarewk/kubernetes-cluster.git

Problem pojawił się również dla submodułów gita (_.gitmodules_).

### Atrybut wykonywalności skryptów

W konfiguracji uczelnianej git nie ustawia domyślnie atrybutu wykonalności
dla plików wykonywalnych i zdejmuje go przy aktualizacji pliku.
Problem rozwiązałem dodaniem komendy `chmod +x bin/*` do skryptu _pull_.

### Konfiguracja dostępu do maszyn bez hasła

Poza konfiguracją _CoreOS_ wypełem konfigurację SSH do bezhasłowego
dostępu. W pliku _~/.ssh/config_ umieściłem:

```
Host s?
  User admin
  
IdentityFile ~/.ssh/id_rsa
IdentitiesOnly yes

Host s?
  StrictHostKeyChecking no
  UserKnownHostsFile /dev/null
```

### Problemy z siecią

W trakcie pierwszego uruchamiania występowały problemy z siecią uczelnianą, więc
rozszerzyłem plik _ansible.cfg_ o ponawianie prób wywoływania komend dodając
wpis _retires=5_ do sekcji _[ssh_connection]_.

### Limit 3 serwerów DNS
Napotkałem [limit 3 serwerów DNS](https://github.com/kubernetes-incubator/kubespray/blob/master/docs/dns-stack.md#limitations):
```
TASK [docker : check system nameservers] **************************************
Friday 26 January 2018  14:47:09 +0100 (0:00:01.429)       0:04:26.879 ******** 
ok: [node3] => {"changed": false, "cmd": "grep \"^nameserver\" /etc/resolv.conf | sed 's/^nameserver\\s*//'", "delta": "0:00:00.004652", "end": "2018-01-26 13:47:11.659298", "rc": 0, "start": "2018-01-26 13:47:11.654646", "stderr": "", "stderr_lines": [], "stdout": "172.29.146.3\n1
72.29.146.6\n10.146.146.3\n10.146.146.6", "stdout_lines": ["172.29.146.3", "172.29.146.6", "10.146.146.3", "10.146.146.6"]}
...
TASK [docker : add system nameservers to docker options] **********************
Friday 26 January 2018  14:47:13 +0100 (0:00:01.729)       0:04:30.460 ******** 
ok: [node3] => {"ansible_facts": {"docker_dns_servers": ["10.233.0.3", "172.29.146.3", "172.29.146.6", "10.146.146.3", "10.146.146.6"]}, "changed": false}
...
TASK [docker : check number of nameservers] ***********************************
Friday 26 January 2018  14:47:15 +0100 (0:00:01.016)       0:04:32.563 ******** 
fatal: [node3]: FAILED! => {"changed": false, "msg": "Too many nameservers. You can relax this check by set docker_dns_servers_strict=no and we will only use the first 3."}
```

Okazało się, że maszyna _s8_ była podłączona również na drugim interfejsie
sieciowym, w związku z tym miała zbyt dużo wpisów serwerów DNS.

Rozwiązałem problem ręcznie logując się na maszynę i wyłączając drugi
interfejs sieciowy komendą `ip l set eno1 down`.


## Pierwszy dzień - uruchamianie skryptów z maszyny s6
  
Większość przeszkód opisałem w powyższym rozdziale, więc w tym skupię się
tylko na problemach związanych z pierwszą próbą uruchomienia skryptów na 
maszynie s6.

Najpierw próbowałem uruchomić skrypty na maszynach: s2, s4 i s5
```bash
cd ~/kubernetes/kubernetes-cluster
bin/setup-cluster-full 10.146.255.{2,4,5}
```

Po uruchomieniu okazało się, że maszyna _s2_ posiada tylko połowę RAMu (4GB) i
nie  mieszczą się na niej obrazy Dockera konieczne do uruchomienia klastra.

Kolejną próbą było uruchomienie na maszynach s4, s5, s8 i s9. Skończyło się
problemami z Vaultem opisanymi w dalszych rozdziałach.


## Kolejne próby uruchamiania klastra z maszyny s2

Dalsze testy przeprowadzałem na maszynach: s4, s5, s6, s8 i s9.

Najwięcej czasu spędziłem na rozwiązaniu problemu z DNSami opisanym wyżej.

### Generowanie inventory z HashiCorp Vault'em

Skrypt _inventory_builder.py_ z _Kubespray_ generuje wpisy oznaczające węzły
jako posiadające HashiCorp Vaulta.

Uruchomienie z Vault'em zakończyło się błędem, więc wyłączyłem Vault'a
rozbijając skrypt _bin/setup-cluster-full_ na krok konfiguracji i krok
uruchomienia, pomiędzy którymi mogłem wyedytować _inventory/inventory.cfg_:
```bash
    bin/setup-cluster-configure 10.146.255.{4,5,6,8,9}
    bin/setup-cluster
```

Próbowałem dostosować parametr [_cert_management_](https://github.com/kubernetes-incubator/kubespray/blob/master/docs/vault.md), 
żeby działał zarówno z _Vaultem_ jak i bez, ale nie dało to żadnego skutku.
Objawem było nie uruchamianie się _etcd_.

Uznałem, że taka konfiguracja jeszcze nie działa i zarzuciłem dalsze próby.
Aby rozwiązać problem trzeba usunąć wpisy pod kategorią _[vault]_ z pliku
_inventory.cfg_. 

### Niepoprawne znajdowanie adresów IP w ansible

Z jakiegoś powodu konfiguracje _s6_ (node3) i _s8_ (node4) kończyły się błędem:
```
TASK [kubernetes/preinstall : Stop if ip var does not match local ips] ********
Friday 26 January 2018  16:37:48 +0100 (0:00:01.297)       0:00:48.587 ********
fatal: [node4]: FAILED! => {
    "assertion": "ip in ansible_all_ipv4_addresses",
    "changed": false,
    "evaluated_to": false
}
fatal: [node3]: FAILED! => {
    "assertion": "ip in ansible_all_ipv4_addresses",
    "changed": false,
    "evaluated_to": false
}
```
Trzy dni później nie wprowadzając po drodze żadnych zmian uruchomiłem klaster
bez problemu.

Przyczyną błędu okazały się pozostałości konfiguracji maszyn niezależne ode mnie.

### Dostęp do _Kubernetes Dashboardu_

_Kubernetes Dashboard_ jest dostępny pod poniższą ścieżką HTTP:

    /api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/service/default/kubernetes

Można się do niego dostać na dwa sposoby:

1. `kubectl proxy`, które wystawia dashboard na adresie _http://127.0.0.1:8001_
2. Pod adresem _https://10.146.225.4:6443_, gdzie _10.146.225.4_ to adres IP
  dowolnego mastera, w tym przypadku maszyny _s4_
  
Kompletne adresy to:
 
    http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/service/default/kubernetes
    https://10.146.225.4:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/service/default/kubernetes

#### Przekierowanie portów
Jeżeli nie pracujemy z maszyny uczelnianej porty możemy przekierować przez SSH
na następujące sposoby (jeżeli skrypty uruchamialiśmy z maszyny _s2_ i łączymy
się do mastera na maszynie _s4_):

1. Plik _~/.ssh/config_:
  ```
  Host s2
    LocalForward 127.0.0.1:8001 localhost:8001
    LocalForward 127.0.0.1:6443 10.146.225.4:6443
  ```
2. Argumenty ssh, np.: 
  ```
  ssh -L 8001:localhost:8001 -L 6443:10.146.225.4:6443 nazarewk@s2
  ```


#### Użytkownik i hasło
Domyślna nazwa użytkownika Dashboardu to _kube_, a hasło znajduje się w pliku
_credentials/kube_user_.

W starszej wersji (uruchamianej wcześniej) _Kubernetes_ i/lub _Kubespray_ brakowało
opcji logowania przy pomocy nazwy użytkownika i hasła:

![Brakujący login](assets/dashboard-login-old.png){width=500 height=240}\
Od 29 stycznia 2018 roku widzę poprawny ekran logowania (opcja Basic):

![Brakujący login](assets/dashboard-login-new.png){width=500 height=445}\

### Instalacja dodatkowych aplikacji z użyciem Kubespray

_Kubespray_ ma wbudowaną instalację kilku dodatkowych aplikacji
playbookiem _upgrade-cluster.yml_ z tagiem _apps_ (skrypt _bin/setup-cluster-upgrade_).

Zmieniłem _kube_script_dir_ na lokalizacje z poza _/usr/local/bin_, 
bo w systemie _CoreOS_ jest read-only squashfsem, wybrałem _/opt/bin_ ponieważ
znajdował się już w _PATHie_ na _CoreOS_. Później dowiedziałem się, że domyślnie
zmiany _CoreOS_ powinny być umieszczane w folderze _/opt_

W końcu ze względu na liczne błędy zarzuciłem temat.

### Instalacja _Helm_

[_Helm_](https://github.com/kubernetes/helm) jest menadżerem pakietów dla
_Kubernetes_. Jego głównym zadaniem jest standaryzacja, automatyzacja i ułatwienie
instalacji aplikacji w _Kubernetes_.

_Helm_ składa się z:

- programu _helm_ uruchamianego lokalnie i korzystającego z danych dostępowych
  _kubectla_,
- aplikacji serwerowej _Tiller_, z którą _helm_ prowadzi interakcje,
- pakietów _Charts_ i ich repozytoriów, domyślnie jest to [_kubernetes/charts_](https://github.com/kubernetes/charts),

Jego instalacja sprowadza się do:

1. ściągnięcia pliku wykonywalnego dla obecnej architektury,
2. dodania roli RBAC dla _Tillera_,
3. Wywołanie komendy `helm init --service-account tiller`

Wszystkie kroki zawierają się w skrypcie _bin/install-helm_.
Ze względu na braku dystrybucji _Helm_ na _FreeBSD_ całość uruchamiam przez SSH
na węźle-zarządcy (domyślnie _s4_).

Szybko okazało się, że większość pakietów wymaga trwałych zasobów dyskowych i
nie uda się ich uruchomić bez ich konfiguracji w sieci uczelnianej.

